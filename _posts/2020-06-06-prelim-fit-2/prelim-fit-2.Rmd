---
title: "prelim-fit-2"
description: |
  In this post we used random forest to train a regression model that uses a set of predictors to predict individual score. The details of feature selection and training the model (including choosing and tuning hyperparmater) will be discussed in a seperate post. To summarize the performance of the random forest regression model: the OOB RMSE for the full training set is 92.48828, the OOB RMSE for the testing set is 100.5576
author:
  - name: Peeta Li
    url: https://example.com/norajones
date: 06-06-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      cache = TRUE,
                      echo = TRUE,
                      cache.lazy = FALSE)
```

Load up our packages

```{r}
library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(rio)
library(vip)
library(rpart.plot)
library(tictoc)
library(future)
theme_set(theme_minimal())
```

read in data - The data consists of 3 parts, 

```{r, linewidth = 40}

frl <- import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
              setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

stu_counts <- import(
  "https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv", 
  setclass = "tbl_df")  %>% 
  filter(state == "OR" & year == 1718)  %>% 
  count(ncessch, wt = n)  %>% 
  mutate(ncessch = as.double(ncessch))

frl <- left_join(frl, stu_counts)

frl <- frl %>% 
    mutate(p_free_lunch = free_lunch_qualified/n,
          p_reduced_price = reduced_price_lunch_qualified/n)

# read in train.csv
math <- read_csv(here::here('data','train.csv')) %>%
  select(-classification) %>% sample_frac(.01)

# read in xlsx
sheets <- readxl::excel_sheets(here::here('data','fallmembershipreport_20192020.xlsx'))
ode_schools <- readxl::read_xlsx(here::here('data','fallmembershipreport_20192020.xlsx'),
                                 sheet = sheets[4])
ethnicities <- ode_schools %>% 
  select(attnd_schl_inst_id = `Attending School ID`,
         attnd_dist_inst_id = `Attending District Institution ID`,
         sch_name = `School Name`,
         contains('%')) %>%
  janitor::clean_names()

names(ethnicities) <- gsub('x2019_20_percent', 'p', names(ethnicities))
# join the two data sets
math <- left_join(math, ethnicities)


df <- math %>% 
    left_join(frl)


```

Split the data to training and testing sets and do 10 folds CV within training set. 

```{r}
# split to training and testing
df_split <- initial_split(df, strata = "score")

# specify training and testing sets
df_train <- training(df_split) 
df_test <- testing(df_split)

# cross-validation
df_cv <- vfold_cv(df_train,  strata = "score")

```

Specfify the recipy recipe

```{r}
rf_rec <- recipe(score ~ ., df_train) %>%
  step_mutate(tst_dt = as.numeric(tst_dt)) %>%
  step_rm(contains('bnch')) %>%
  update_role(contains('id'), ncessch, sch_name, new_role = 'id') %>%
  step_novel(all_nominal())  %>%
  step_unknown(all_nominal(), -all_outcomes()) %>%
  step_medianimpute(all_numeric()) %>%
  step_dummy(all_nominal(), -has_role(match = 'id'), -all_outcomes()) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)
  
rf_rec %>% prep()
```

Specify the random forest model and and grid for tuning hyperparameter. 

```{r}
library(ranger)
(cores <- parallel::detectCores())
mod_ran <- 
  rand_forest() %>%
  set_engine('ranger',
             num.threads = 7,
             importance = 'permutation',
             verbose = T) %>%
  set_mode('regression') %>%
  set_args(trees = 1000,
           mtry = tune(),
           min_n = tune())

ran_params  <- parameters(mtry(range = c(1,55)), min_n(range = c(1,80)))
ran_grid <- grid_max_entropy(ran_params, size = 20)

ran_grid %>% 
  ggplot(aes(mtry, min_n)) +
  geom_point()

```

Initiate workflow

```{r}
wf_ran <-  workflow() %>%
  add_model(mod_ran) %>%
  add_recipe(rf_rec)
```

Tune the model with the gird just made 

```{r, cache = TRUE}
metrics_eval <- metric_set(rmse)
plan(multisession)
tic()
tune_ran <- tune_grid(
  wf_ran,
  df_cv,
  grid = ran_grid,
  metrics = metrics_eval,
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x))
toc()

plan(sequential)
```


```{r}
tune_ran %>%
  collect_metrics(summarize = T) %>%
  ggplot(aes(mtry, mean, color = factor(min_n))) + 
  geom_point() +
  ylab('avg_rmse') + 
  ggtitle('Tunning hyperparameters')
```

Update model and make predictions to kaggle test set 

```{r}
## updata the new model
ran_best <- tune_ran %>%
  show_best('rmse', n = 1)

mod_ran_final <- mod_ran %>%
  finalize_model(ran_best)

wf_final <- workflow() %>%
  add_model(mod_ran_final) %>%
  add_recipe(rf_rec)

# fit full train
test_mod <- fit(wf_final, data = df_train)
# OOB rmse for full training 
sqrt(test_mod$fit$fit$fit$prediction.error)

# prep and bake test
test <- fit(wf_final, data = df_test)
# OOB rmse for testing 
sqrt(test$fit$fit$fit$prediction.error)
```

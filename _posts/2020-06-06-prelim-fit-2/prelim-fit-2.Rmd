---
title: "prelim fit 2"
description: |
  In this post we used random forest to train a regression model that the final analytic dataset to predict individual math score. 
author:
  - name: Peeta Li, Yufei Zhao, Bernice Cheung
date: 06-06-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      cache = FALSE,
                      echo = TRUE,
                      cache.lazy = FALSE)
```

## Description of the models fit 

* Describe each model fit
    * We used random forest to build a regression model based on the final analytic dataset we constructed. The details of how we built up our final analytic dataset could be found in the description-of-the-data post.
* Why the given model was selected
    * Random forest was chosen because it usually provides higher accuracy. Importantly, it also has the least variability in prediction accuracy. 
* Hyperparameters to be optimized
    * As will be discussed below. We chose to tune two hyperparameters. The m_try, which indicates the number of features to extract for each bootstrap. The min_n, which indicates the minimun nubmer of observation in the terminal node. The hyperparameters were tune by grid serach. Details were discussed below
* Assumptions of the model
    * The random forest model does not have formal distributional assumption because it is non-parametric. One assumption it might have is that when bootstrap was used for creating each sub tree, it decorelates the trees. Such that the predictors chosen each sub tree is independent from other trees. 
* A high-level (think broad audience) description of what the model is doing and why it is appropriate (even as an initial starting point)
    * The basis of random forest is the binary decision tree, with each node being a predictor. The order of the split was optimized by minimizing SSE at each split. Following the tree, when one reaches the terminal node, one can get the decision tree predicted score or class. In random forest, to minimize variance, it has multiple (e.g., 1000) decision trees, with each tree having a subset predictors. In this way, between-tree correlation is minimized and the averaged score or mode class has the least variance. By tuning a random forest model, one can decide the best percentage of the predictors one should extract for building each sub try, the depth of each sub tree and the bst split for each sub tree. Then the random forest model can be used to make predictions for new data. 
* How we evaluate model performance
    * We evaluated the random forest model performance by looking at RMSE during cross-validation and the out-of-bag RMSE when fitting the whole training/testing splits. 

## Model fits

### Load up our packages

```{r}
library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(rio)
library(vip)
library(rpart.plot)
library(tictoc)
library(future)
theme_set(theme_minimal())
```

### Read in the final merged training data, the exact percedure of the selecting each variables in the merged training data can be found in a the post of the description of the data. 

```{r}
# read in the final merged data
df <- import(here::here('data','final_merged_train.csv')) %>%
  sample_frac(.05)
```

### Split the data to training and testing sets and do 10 folds CV within training set. 

```{r}
# split to training and testing
df_split <- initial_split(df, strata = "score")

# specify training and testing sets
df_train <- training(df_split) 
df_test <- testing(df_split)

# cross-validation
df_cv <- vfold_cv(df_train,  strata = "score")
```

### Specfify the recipe. For the data engineering part, we did the following (in order): \

* Change the variable 'tst_dt' to dates. \
* Made a new numeric variable name 'tst_dt_num', which is a numeric version of 'tst_dt', is a predictor. \
* Update the role of 'tst_dt' to 'time_index', thus not a predictor anymore. \
* Update the id variables to the new role 'id', thus not a predictor anymore. \
* Based on our exploration of categorical variables in the description-of-the-data post, many categorical variables have only a few  observations for one or two of their levels. It is possible that the fraction we chose for building the model won't choose any observations in that level, which may cause some problems when making predictions about the testing set. Thus we use step_novel to prevent this from happening. \
* For nominal variables, we changed all NAs to a new level called unknown. \
* For numeric variables, we removed all NAs by impute the median value of the varaible. \
* Remove varaibles that has zero variance. \
* Dummy code all nominal variables. \
* In case dummy coding introduce any other variables that have zero variance. \
* prep the training data with the recipe to make sure everything flows. \

```{r}

rf_rec <- recipe(score ~ ., df_train) %>%
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
              tst_dt_num = as.numeric(tst_dt)) %>%
  update_role(tst_dt, new_role = "time_index")  %>% 
  update_role(contains('id'), ncessch, sch_name, new_role = 'id') %>%
  step_novel(all_nominal())  %>%
  step_unknown(all_nominal(), -all_outcomes()) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role('id')) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% 
  step_dummy(all_nominal(), -has_role(match = 'id'), -tst_dt) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) 
  
check <- rf_rec %>% prep()
```

### Specify the random forest model and and grid for tuning hyperparameter. \

For the random forest model, there are three possible hyperparameters. We set the number of trees to be 1000, meaning that the bootstrap procedure will be repeated 1000 times. We then tuned the hyperparameters of mtry (i.e., the number of precitors to extract in each bootstrap) and min_n (the minimum number of observation left in the terminal node), determinating the depth of the tree. \
We did a grid search. We built the grid with max entropy. The range of mtry is from 1 to the total number of predictors (i.e., 56) and the range of min_is from 1 to 100

```{r}
library(ranger)

mod_ran <- 
  rand_forest() %>%
  set_engine('ranger',
             num.threads = 7,
             importance = 'permutation',
             verbose = T) %>%
  set_mode('regression') %>%
  set_args(trees = 1000,
           mtry = tune(),
           min_n = tune())

ran_params  <- parameters(mtry(range = c(1,56)), min_n(range = c(1,100)))
ran_grid <- grid_max_entropy(ran_params, size = 20)
```

### Initiate workflow

```{r}
wf_ran <-  workflow() %>%
  add_model(mod_ran) %>%
  add_recipe(rf_rec)
```

### Tune the model

```{r, cache = FALSE}
metrics_eval <- metric_set(rmse)
plan(multisession)
tic()
tune_ran <- tune_grid(
  wf_ran,
  df_cv,
  grid = ran_grid,
  metrics = metrics_eval,
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x))
toc()

plan(sequential)
```

We used RMSE as our metric. We averaged the RMSE measure across all 10 fold for a pair of hyperparameters. The following graph shows the relationshi between fold-averaged RMSE and each pair of hyperparameters. 

```{r}
tune_ran %>%
  collect_metrics(summarize = T) %>%
  ggplot(aes(mtry, mean, color = factor(min_n))) + 
  geom_point() +
  ylab('avg_rmse') + 
  ggtitle('Tunning hyperparameters')
```

### Update the model with the best performed hyperparameters. Then update the workflow as well. 

```{r}
## updata the new model
ran_best <- tune_ran %>%
  show_best('rmse', n = 1)

mod_ran_final <- mod_ran %>%
  finalize_model(ran_best)

wf_final <- workflow() %>%
  add_model(mod_ran_final) %>%
  add_recipe(rf_rec)

ran_best
```

### Fit the whole training split and test on the test split

```{r}
# final fit 
finalized_fit <- last_fit(
  wf_final,
  split = df_split)

finalized_fit$.metrics
```

### make predictions about the new testing set 

```{r, eval = TRUE}
real_test <-  read_csv(here::here("data", "final_merged_test.csv"),
                       col_types = cols(.default = col_guess())) 

# fit full training 
full_train_fit <- fit(wf_final,df_train)

# make prediction:
preds <- predict(full_train_fit, new_data = real_test)


output <- tibble(Id = real_test$id, Predicted = preds$.pred) 
write_csv(output, here::here('data','predict','preds-prelim2.csv'))
```


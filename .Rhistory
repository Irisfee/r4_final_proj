# create a workflow with the model and recipy set above
wf_df <- workflow() %>%
add_recipe(rec) %>%
add_model(mod)
# fit the model with default hyperparameters to the resampled data
fit_default <- fit_resamples(wf_df, cv)
# evaluate the performance
collect_metrics(fit_default)
cl <- makeCluster(8)
registerDoParallel(cl)
# incorporate the updated hyperparameters to the workflow
tune_lr <- mod %>%
set_args(trees = 5000,
learn_rate = tune(),
stop_iter = 20,
validation = 0.2)
wf_tune_lr <- wf_df %>%
update_model(tune_lr)
# set the tuning grid for learning rate
grd <- expand.grid(learn_rate = seq(0.005, 0.3, length.out = 20))
# fit the model
tune_tree_lr <- tune_grid(wf_tune_lr, cv, grid = grd)
tune_tree_lr
knitr::opts_chunk$set(echo = FALSE,
cache = TRUE)
library(tidyverse)
library(tidymodels)
library(doParallel)
library(skimr)
library(xgboost)
library(future)
set.seed(060820)
full_train <- read_csv(here::here('data',"final_merged_train.csv"),
col_types = cols(.default = col_guess())) %>%
sample_frac(.01)
splt <- initial_split(full_train, strata = "score")
train <- training(splt)
test <- testing(splt)
cv <- vfold_cv(train)
# set a recipe for prepping the data
rec <- recipe(score ~ ., train) %>%
step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
tst_dt_num = as.numeric(tst_dt)) %>%
update_role(tst_dt, new_role = "time_index")  %>%
update_role(contains('id'), ncessch, sch_name, new_role = 'id') %>%
step_novel(all_nominal())  %>%
step_unknown(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_numeric(), -all_outcomes(), -has_role('id')) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
step_dummy(all_nominal(), -has_role(match = 'id'), -tst_dt) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)
# check the recipe by applying it to the training data
check <- rec %>% prep()
# specify a stochastic gradient boosting tree model
mod <- boost_tree() %>%
set_engine("xgboost", nthread = 8) %>%
set_mode("regression")
# create a workflow with the model and recipy set above
wf_df <- workflow() %>%
add_recipe(rec) %>%
add_model(mod)
# fit the model with default hyperparameters to the resampled data
fit_default <- fit_resamples(wf_df, cv)
# evaluate the performance
collect_metrics(fit_default)
cl <- makeCluster(8)
registerDoParallel(cl)
# incorporate the updated hyperparameters to the workflow
tune_lr <- mod %>%
set_args(trees = 5000,
learn_rate = tune(),
stop_iter = 20,
validation = 0.2)
wf_tune_lr <- wf_df %>%
update_model(tune_lr)
# set the tuning grid for learning rate
grd <- expand.grid(learn_rate = seq(0.005, 0.3, length.out = 20))
# fit the model
tune_tree_lr <- tune_grid(wf_tune_lr, cv, grid = grd)
stopCluster(cl)
cl <- makeCluster(8)
registerDoParallel(cl)
# update the workflow with optimized learning rate and tuned tree_depth
tune_depth <- tune_lr %>%
finalize_model(select_best(tune_tree_lr, "rmse")) %>%
set_args(tree_depth = tune())
wf_tune_depth <- wf_df %>%
update_model(tune_depth)
# set the tuning grid for tree_depth
grd <- expand.grid(tree_depth = seq(1, 8, 1))
# fit the model
tune_tree_depth <- tune_grid(wf_tune_depth, cv, grid = grd)
stopCluster(cl)
cl <- makeCluster(8)
registerDoParallel(cl)
# update the workflow with optimized tree depth and tuned loss_reduction
tune_reg <- tune_depth %>%
finalize_model(select_best(tune_tree_depth, "rmse")) %>%
set_args(loss_reduction = tune())
wf_tune_reg <- wf_df %>%
update_model(tune_reg)
# set the tuning grid for loss_reduction
grd <- expand.grid(loss_reduction = seq(1, 20, 2))
# fit the model
tune_tree_reg <- tune_grid(wf_tune_reg, cv, grid = grd)
stopCluster(cl)
cl <- makeCluster(8)
registerDoParallel(cl)
# update the workflow with optimized tuned loss_reduction and tuned mtry & sample_size
tune_rand <- tune_reg %>%
finalize_model(select_best(tune_tree_reg, "rmse")) %>%
set_args(mtry = tune(),
sample_size = tune())
wf_tune_rand <- wf_df %>%
update_model(tune_rand)
# set the tuning grid matrix for both variable
grd <- grid_max_entropy(finalize(mtry(), juice(prep(rec))),
sample_size = sample_prop(),
size = 20)
# fit the model
tune_tree_rand <- tune_grid(wf_tune_rand, cv, grid = grd)
stopCluster(cl)
show_best(tune_tree_rand, "rmse", n=1)
tune_final <- tune_rand %>%
finalize_model(select_best(tune_tree_rand, "rmse"))
wf_final <- wf_df %>%
update_model(tune_final)
registerDoSEQ()
test_mod <- last_fit(
wf_final,
split = splt)
test_mod$.metrics
full_train_fit <- fit(wf_final, score ~ ., full_train)
real_test <-  read_csv(here::here("data", "final_merged_test.csv"),
col_types = cols(.default = col_guess()))
real_test <-  read_csv(here::here("data", "final_merged_test.csv"),
col_types = cols(.default = col_guess()))
full_train_fit <- fit(wf_final, score ~ ., full_train)
full_train_fit <- fit(wf_final, score ~ .,train  )
full_train_fit <- fit(wf_final,train )
full_train_fit
# make prediction:
preds <- predict(full_train_fit, new_data = real_test)
write_csv(output, here::here('data','predict','preds-prelim3.csv')
)
output <- tibble(Id = kaggle_test_prep$id, Predicted = preds$.pred)
write_csv(output, here::here('data','predict','preds-prelim3.csv'))
output <- tibble(Id = real_test$id, Predicted = preds$.pred)
write_csv(output, here::here('data','predict','preds-prelim3.csv'))
knitr::opts_chunk$set(message = FALSE,
warning = FALSE,
cache = F,
echo = TRUE,
cache.lazy = FALSE)
library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(rio)
library(vip)
library(rpart.plot)
library(tictoc)
library(future)
theme_set(theme_minimal())
# read in the final merged data
df <- import(here::here('data','final_merged_train.csv')) %>%
sample_frac(.05)
# split to training and testing
df_split <- initial_split(df, strata = "score")
# specify training and testing sets
df_train <- training(df_split)
df_test <- testing(df_split)
# cross-validation
df_cv <- vfold_cv(df_train,  strata = "score")
rf_rec <- recipe(score ~ ., df_train) %>%
step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
tst_dt_num = as.numeric(tst_dt)) %>%
update_role(tst_dt, new_role = "time_index")  %>%
update_role(contains('id'), ncessch, sch_name, new_role = 'id') %>%
step_novel(all_nominal())  %>%
step_unknown(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_numeric(), -all_outcomes(), -has_role('id')) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
step_dummy(all_nominal(), -has_role(match = 'id'), -tst_dt) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)
check <- rf_rec %>% prep()
library(ranger)
mod_ran <-
rand_forest() %>%
set_engine('ranger',
num.threads = 7,
importance = 'permutation',
verbose = T) %>%
set_mode('regression') %>%
set_args(trees = 1000,
mtry = tune(),
min_n = tune())
ran_params  <- parameters(mtry(range = c(1,56)), min_n(range = c(1,100)))
ran_grid <- grid_max_entropy(ran_params, size = 20)
wf_ran <-  workflow() %>%
add_model(mod_ran) %>%
add_recipe(rf_rec)
metrics_eval <- metric_set(rmse)
plan(multisession)
tic()
tune_ran <- tune_grid(
wf_ran,
df_cv,
grid = ran_grid,
metrics = metrics_eval,
control = control_resamples(verbose = TRUE,
save_pred = TRUE,
extract = function(x) x))
toc()
plan(sequential)
tune_ran %>%
collect_metrics(summarize = T) %>%
ggplot(aes(mtry, mean, color = factor(min_n))) +
geom_point() +
ylab('avg_rmse') +
ggtitle('Tunning hyperparameters')
## updata the new model
ran_best <- tune_ran %>%
show_best('rmse', n = 1)
mod_ran_final <- mod_ran %>%
finalize_model(ran_best)
wf_final <- workflow() %>%
add_model(mod_ran_final) %>%
add_recipe(rf_rec)
# final fit
finalized_fit <- last_fit(
wf_final,
split = df_split)
finalized_fit$.metrics
real_test <-  read_csv(here::here("data", "final_merged_test.csv"),
col_types = cols(.default = col_guess()))
# fit full training
full_train_fit <- fit(wf_final,df_train)
# make prediction:
preds <- predict(full_train_fit, new_data = real_test)
output <- tibble(Id = real_test$id, Predicted = preds$.pred)
write_csv(output, here::here('data','predict','preds-prelim2.csv'))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rio)
library(vip)
library(skimr)
library(janitor)
# Original training set
train <- import(here::here('data','train.csv')) %>%
mutate(ncessch = as.double(ncessch)) %>%
select(-classification) %>%
mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd),
ayp_lep = ifelse(is.na(ayp_lep), "G", ayp_lep))
dic <- import(here::here('data', 'data_dictionary.csv'))
knitr::kable(dic)
# add in extra data
# 1) staff
staff <- import(here::here('data','extra_data','staff','ccd_sch_059_1718_l_1a_083118.csv')) %>%
filter(ST == "OR")  %>%
select(NCESSCH, TEACHERS) %>% # Teachers: number of teachers (numeric)
janitor::clean_names() %>%
mutate(ncessch = as.double(ncessch))
# join train and staff by ncessch
df <- train %>%
left_join(staff)
# 2) scool characteristics
# clean the school characteristic dataset
schoolDf <- import(here::here('data','extra_data','school_characteristics','ccd_sch_129_1718_w_1a_083118.csv'))%>%
filter(ST == "OR") %>%
janitor::clean_names() %>%
mutate(ncessch = as.double(ncessch)) %>%
modify_if(is.character, as.factor)
# Merge with the full dataset
fullDf <- left_join(train, schoolDf, by= "ncessch") %>%
select(score, colnames(schoolDf))
# Plot bar graph to explore the data
expl <- names(fullDf[,2:21])
# func for map
explore_fun <- function(x, y) {
ggplot(fullDf, aes(x = .data[[x]], y = .data[[y]]) ) +
geom_bar(stat="identity") +
theme_minimal()
}
elev_plots <- map(expl, ~explore_fun(.x, "score"))
school_char <- import(here::here('data','extra_data','school_characteristics','ccd_sch_129_1718_w_1a_083118.csv')) %>%
filter(ST == "OR")  %>%
select(NCESSCH, TITLEI_STATUS, NSLP_STATUS, VIRTUAL) %>%
janitor::clean_names()  %>%
mutate(ncessch = as.double(ncessch))
# join df and school_char by ncessch
df <- df %>%
left_join(school_char)
# 3) lunch eligibility
lunch_elig <- import(here::here('data','extra_data','lunch_program_eligibility','ccd_sch_033_1718_l_1a_083118.csv')) %>%
filter(ST == "OR")  %>%
select(NCESSCH, LUNCH_PROGRAM, STUDENT_COUNT)  %>%
janitor::clean_names()  %>%
select(ncessch, lunch_program, student_count)  %>%
mutate(student_count = replace_na(student_count, 0))  %>%
pivot_wider(names_from = lunch_program,
values_from = student_count)  %>%
janitor::clean_names()  %>%
mutate(ncessch = as.double(ncessch))
stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv",
setclass = "tbl_df")  %>%
filter(state == "OR" & year == 1718)  %>%
count(ncessch, wt = n)  %>%
mutate(ncessch = as.double(ncessch))
lunch_elig <- left_join(lunch_elig, stu_counts)
lunch_elig <- lunch_elig %>%
mutate(free_lunch_prop = free_lunch_qualified / n,
reduced_lunch_prop = reduced_price_lunch_qualified / n)  %>%
select(ncessch, ends_with("prop"))
df <- df %>%
left_join(lunch_elig)
# 4) directory
directory <- import(here::here('data','extra_data','directory','ccd_sch_029_1718_w_1a_083118.csv')) %>%
filter(ST == "OR")  %>%
select(NCESSCH, SCH_TYPE_TEXT, CHARTER_TEXT) %>%
janitor::clean_names() %>%
mutate(ncessch = as.double(ncessch))
df <- df %>%
left_join(directory)
# 5) membership
sheets <- readxl::excel_sheets(here::here('data','extra_data','fallmembershipreport_20192020.xlsx'))
ode_schools <- readxl::read_xlsx(here::here('data','extra_data','fallmembershipreport_20192020.xlsx'),
sheet = sheets[4])
ethnicities <- ode_schools %>%
select(attnd_schl_inst_id = `Attending School ID`,
attnd_dist_inst_id = `Attending District Institution ID`,
sch_name = `School Name`,
contains('%')) %>%
janitor::clean_names()
names(ethnicities) <- gsub('x2019_20_percent', 'p', names(ethnicities))
df <- df %>%
left_join(ethnicities)
df <- df%>%
modify_if(is.character, as.factor)
(df_skim <- skim(df))
test <- import(here::here('data','test.csv')) %>%
mutate(ncessch = as.double(ncessch)) %>%
select(-classification) %>%
mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd),
ayp_lep = ifelse(is.na(ayp_lep), "G", ayp_lep))
test <- import(here::here('data','test.csv')) %>%
mutate(ncessch = as.double(ncessch)) %>%
mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd),
ayp_lep = ifelse(is.na(ayp_lep), "G", ayp_lep))
# join staff dataser
test_full <- test %>%
left_join(staff)
# join school characteristic dataset
test_full <- test_full %>%
left_join(school_char)
# join lunch eligibility dataset
test_full <- test_full %>%
left_join(lunch_elig)
# join directory dataset
test_full <- test_full %>%
left_join(directory)
# join membership dataset
test_full <- test_full %>%
left_join(ethnicities)
test_full <- test_full%>%
modify_if(is.character, as.factor)
test_full <- test_full %>%
select(-calc_admn_cd)
write.csv(test_full, here::here('data','final_merged_test.csv'), row.names = F)
knitr::opts_chunk$set(message = FALSE,
warning = FALSE,
cache = FALSE,
echo = TRUE,
cache.lazy = FALSE)
library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(rio)
library(vip)
library(rpart.plot)
library(tictoc)
library(future)
theme_set(theme_minimal())
# read in the final merged data
df <- import(here::here('data','final_merged_train.csv')) %>%
sample_frac(.05)
# split to training and testing
df_split <- initial_split(df, strata = "score")
# specify training and testing sets
df_train <- training(df_split)
df_test <- testing(df_split)
# cross-validation
df_cv <- vfold_cv(df_train,  strata = "score")
rf_rec <- recipe(score ~ ., df_train) %>%
step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
tst_dt_num = as.numeric(tst_dt)) %>%
update_role(tst_dt, new_role = "time_index")  %>%
update_role(contains('id'), ncessch, sch_name, new_role = 'id') %>%
step_novel(all_nominal())  %>%
step_unknown(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_numeric(), -all_outcomes(), -has_role('id')) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
step_dummy(all_nominal(), -has_role(match = 'id'), -tst_dt) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)
check <- rf_rec %>% prep()
library(ranger)
mod_ran <-
rand_forest() %>%
set_engine('ranger',
num.threads = 7,
importance = 'permutation',
verbose = T) %>%
set_mode('regression') %>%
set_args(trees = 1000,
mtry = tune(),
min_n = tune())
ran_params  <- parameters(mtry(range = c(1,56)), min_n(range = c(1,100)))
ran_grid <- grid_max_entropy(ran_params, size = 20)
wf_ran <-  workflow() %>%
add_model(mod_ran) %>%
add_recipe(rf_rec)
metrics_eval <- metric_set(rmse)
plan(multisession)
tic()
tune_ran <- tune_grid(
wf_ran,
df_cv,
grid = ran_grid,
metrics = metrics_eval,
control = control_resamples(verbose = TRUE,
save_pred = TRUE,
extract = function(x) x))
knitr::opts_chunk$set(message = FALSE,
warning = FALSE,
cache = TRUE,
echo = TRUE,
cache.lazy = FALSE,
autodep = TRUE)
library(tidyverse)
library(tidymodels)
library(doParallel)
library(skimr)
library(xgboost)
library(future)
set.seed(060820)
full_train <- read_csv(here::here('data',"final_merged_train.csv"),
col_types = cols(.default = col_guess())) %>%
sample_frac(.3)
splt <- initial_split(full_train, strata = "score")
train <- training(splt)
test <- testing(splt)
cv <- vfold_cv(train)
# set a recipe for prepping the data
rec <- recipe(score ~ ., train) %>%
step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
tst_dt_num = as.numeric(tst_dt)) %>%
update_role(tst_dt, new_role = "time_index")  %>%
update_role(contains('id'), ncessch, sch_name, new_role = 'id') %>%
step_novel(all_nominal())  %>%
step_unknown(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_numeric(), -all_outcomes(), -has_role('id')) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
step_dummy(all_nominal(), -has_role(match = 'id'), -tst_dt) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)
# check the recipe by applying it to the training data
check <- rec %>% prep()
# specify a stochastic gradient boosting tree model
mod <- boost_tree() %>%
set_engine("xgboost", nthread = 8) %>%
set_mode("regression")
# create a workflow with the model and recipy set above
wf_df <- workflow() %>%
add_recipe(rec) %>%
add_model(mod)
# fit the model with default hyperparameters to the resampled data
fit_default <- fit_resamples(wf_df, cv)
# evaluate the performance
collect_metrics(fit_default)
full_train <- read_csv(here::here('data',"final_merged_train.csv"),
col_types = cols(.default = col_guess())) #%>%
# sample_frac(.3)
splt <- initial_split(full_train, strata = "score")
train <- training(splt)
test <- testing(splt)
cv <- vfold_cv(train)
# set a recipe for prepping the data
rec <- recipe(score ~ ., train) %>%
step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
tst_dt_num = as.numeric(tst_dt)) %>%
update_role(tst_dt, new_role = "time_index")  %>%
update_role(contains('id'), ncessch, sch_name, new_role = 'id') %>%
step_novel(all_nominal())  %>%
step_unknown(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_numeric(), -all_outcomes(), -has_role('id')) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
step_dummy(all_nominal(), -has_role(match = 'id'), -tst_dt) %>%
step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)
# check the recipe by applying it to the training data
check <- rec %>% prep()
# specify a stochastic gradient boosting tree model
mod <- boost_tree() %>%
set_engine("xgboost", nthread = 8) %>%
set_mode("regression")
# create a workflow with the model and recipy set above
wf_df <- workflow() %>%
add_recipe(rec) %>%
add_model(mod)
# fit the model with default hyperparameters to the resampled data
fit_default <- fit_resamples(wf_df, cv)
# evaluate the performance
collect_metrics(fit_default)
